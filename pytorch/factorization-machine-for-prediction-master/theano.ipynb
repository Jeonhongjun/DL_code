{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FactorizationMachine\n",
    "\n",
    "#### Quick and dirty implementation of Factorization Machines in Python/Theano.\n",
    "\n",
    "#### http://www.libfm.org/\n",
    "\n",
    "#### See original code at :\n",
    "#### https://github.com/instagibbs/FactorizationMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, datasets\n",
    "from sklearn.preprocessing import normalize\n",
    "import theano\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class fact_machine():\n",
    "    def __init__(self, num_feats, k_size=3, ltype=2, lamb=.1):\n",
    "        #num_feats = samples.shape[1]\n",
    "        self.lamb = lamb\n",
    "        self.ltype = ltype\n",
    "        #self.num_samples=num_samples\n",
    "        self.num_feats=num_feats\n",
    "        self.k_size=k_size\n",
    "\n",
    "        w0 = np.random.randn()\n",
    "        self.w0 = theano.shared(value=w0, name='w0')#, borrow=True)\n",
    "\n",
    "        ws = np.random.randn(num_feats)\n",
    "        self.ws = theano.shared(value=ws, name='ws')#, borrow=True)\n",
    "\n",
    "        vs = np.random.randn(num_feats, k_size)\n",
    "        self.vs = theano.shared(value=vs, name='vs')#, borrow=True)\n",
    "\n",
    "        self.input_var = T.matrix()\n",
    "        self.target_var = T.vector()\n",
    "    \n",
    "    #Must be numpy array of float32. Keeps weights, changes other things.\n",
    "    def set_data(self, x, y):\n",
    "        self.shared_x = theano.shared(x)\n",
    "        self.shared_y = theano.shared(y)\n",
    "\n",
    "        self.givens = {\n",
    "          self.input_var : self.shared_x,\n",
    "          self.target_var : self.shared_y\n",
    "        }\n",
    "        self.set_updates()\n",
    "        self.set_train()\n",
    "        self.set_output()\n",
    "    \n",
    "    #Defines the inference-level objective section.\n",
    "    def factorization_objective(self, samples):\n",
    "        yhat = self.w0+T.dot(samples,self.ws)\n",
    "        for i in range(self.num_feats-1):\n",
    "            for j in range(i+1,self.num_feats):\n",
    "                yhat += T.dot(self.vs[i], self.vs[j])*samples[:,i]*samples[:,j]\n",
    "        return yhat\n",
    "    \n",
    "    #The exponential penalty objective outlined in: http://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf\n",
    "    def exp_objective(self):\n",
    "\n",
    "        total_objective = T.log(1+T.exp(-self.target_var*self.factorization_objective(self.input_var)))\n",
    "        if self.ltype == 2:\n",
    "            total_objective += (self.lamb/2)*T.sum(T.sqr(self.ws)) \n",
    "            total_objective += (self.lamb/2)*T.sum(T.sqr(self.vs))\n",
    "        elif self.ltype == 1:\n",
    "            total_objective += self.lamb*T.sum(T.abs_(self.ws))\n",
    "            total_objective += self.lamb*T.sum(T.abs_(self.vs))\n",
    "        else:\n",
    "            raise Exception('Wrong regularization type, must be 1 or 2: ' + str(ltype))\n",
    "        return T.mean(total_objective)\n",
    "    \n",
    "    #SGD formmulation\n",
    "    def gen_updates_sgd(self, loss, learning_rate=.1):\n",
    "        all_parameters = [self.w0, self.ws, self.vs]\n",
    "        all_grads = [theano.grad(loss, param) for param in all_parameters]\n",
    "        updates = []\n",
    "        for param_i, grad_i in zip(all_parameters, all_grads):\n",
    "            updates.append((param_i, param_i - learning_rate * grad_i))\n",
    "        return updates\n",
    "    \n",
    "    def set_updates(self):\n",
    "        updates = self.gen_updates_sgd(self.error())\n",
    "        self.updates = updates\n",
    "  \n",
    "    def error(self):\n",
    "        return self.exp_objective()\n",
    "  \n",
    "    def predict(self):\n",
    "        output = self.factorization_objective(self.input_var)\n",
    "        return output\n",
    "    \n",
    "    def set_train(self):\n",
    "        self.train = theano.function([], self.error(), givens=self.givens, updates=self.updates) \n",
    "    \n",
    "    def set_output(self):\n",
    "        self.output = theano.function([], self.predict(), givens=self.givens, on_unused_input='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\niris = datasets.load_iris() #junk dataset for now\\nsamples = iris.data\\nsamples = normalize(samples)\\nlabels = iris.target\\nlabels[labels > 0] = 1\\nlabels[labels == 0] = -1\\n\\ntrain_x, test_x, train_y, test_y = cross_validation.train_test_split(samples, labels, test_size=0.10)\\ntrain_x_f=train_x.astype(np.float32)\\ntrain_y_f=train_y.astype(np.float32)\\ntest_x_f=test_x.astype(np.float32)\\ntest_y_f=test_y.astype(np.float32)\\n\\n#minibatch_size = 100\\nmb_size = train_x.shape[0]\\nprint mb_size\\nnum_batches = train_x.shape[0]/mb_size\\nrand_vec = np.arange(mb_size)\\n#np.random.shuffle(rand_vec)\\n\\nf_m = fact_machine(train_x.shape[1], mb_size, k_size=0)\\nf_m.set_data(train_x_f, train_y_f)\\n\\n\\nconverged = False\\nlast_perf = 10000000000\\nwhile not converged:\\n  perf_vec =  f_m.train()\\n  print perf_vec\\n  if last_perf <= perf_vec + .000001:\\n    converged = True\\n  last_perf = perf_vec\\n\\nb = 3  \\nprint \"Training Accuracy:\", np.count_nonzero(np.sign(f_m.output()-f_m.w0.eval()) == train_y), len(train_y)\\nf_m.set_data(test_x_f, test_y_f)\\nprint \"Testing Accuracy:\", np.count_nonzero(np.sign(f_m.output()-f_m.w0.eval()) == test_y), len(test_y)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "iris = datasets.load_iris() #junk dataset for now\n",
    "samples = iris.data\n",
    "samples = normalize(samples)\n",
    "labels = iris.target\n",
    "labels[labels > 0] = 1\n",
    "labels[labels == 0] = -1\n",
    "\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(samples, labels, test_size=0.10)\n",
    "train_x_f=train_x.astype(np.float32)\n",
    "train_y_f=train_y.astype(np.float32)\n",
    "test_x_f=test_x.astype(np.float32)\n",
    "test_y_f=test_y.astype(np.float32)\n",
    "\n",
    "#minibatch_size = 100\n",
    "mb_size = train_x.shape[0]\n",
    "print mb_size\n",
    "num_batches = train_x.shape[0]/mb_size\n",
    "rand_vec = np.arange(mb_size)\n",
    "#np.random.shuffle(rand_vec)\n",
    "\n",
    "f_m = fact_machine(train_x.shape[1], mb_size, k_size=0)\n",
    "f_m.set_data(train_x_f, train_y_f)\n",
    "\n",
    "\n",
    "converged = False\n",
    "last_perf = 10000000000\n",
    "while not converged:\n",
    "  perf_vec =  f_m.train()\n",
    "  print perf_vec\n",
    "  if last_perf <= perf_vec + .000001:\n",
    "    converged = True\n",
    "  last_perf = perf_vec\n",
    "\n",
    "b = 3  \n",
    "print \"Training Accuracy:\", np.count_nonzero(np.sign(f_m.output()-f_m.w0.eval()) == train_y), len(train_y)\n",
    "f_m.set_data(test_x_f, test_y_f)\n",
    "print \"Testing Accuracy:\", np.count_nonzero(np.sign(f_m.output()-f_m.w0.eval()) == test_y), len(test_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
