{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tested with :\n",
    "- Linux Ubuntu\n",
    "- Python 3.5\n",
    "- Cuda 8\n",
    "- Conda package for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFER_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = PREFER_CUDA and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if not(use_cuda == PREFER_CUDA):\n",
    "    print('CUDA SETUP NOT AS EXCEPTED')\n",
    "else:\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model assumption\n",
    "- We assume all model variables are binary 0/1 valued\n",
    "- We represent the X input vector has the sparse coding of its \"1\" indices\n",
    "- Indices start at 1 so that we can reserve 0 for padding in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a batch of 2 samples of 3 indices each\n",
    "X = Variable(torch.LongTensor([[11,20,4],[30,10,20],]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, nb_features, dim_embed=50, isClassifier=True, withCuda=True):\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "        \n",
    "        self.nb_features = nb_features\n",
    "        self.dim_embed = dim_embed\n",
    "        self.isClassifier = isClassifier # binary-classifier or regression\n",
    "        \n",
    "        # Stores the bias term\n",
    "        if withCuda: # WARNING : not working at present with CUDA due to type mismatch\n",
    "            self.B = Variable(torch.randn((1)).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "        else:\n",
    "            self.B = Variable(torch.randn((1)).type(torch.FloatTensor), requires_grad=True)\n",
    "        \n",
    "        # Stores the weights for the linear terms\n",
    "        self.embeddingL = nn.Embedding(nb_features, 1, padding_idx=0, max_norm=None, norm_type=2)\n",
    "        \n",
    "        # Stores the weights for the quadratic FM terms\n",
    "        self.embeddingQ = nn.Embedding(nb_features, dim_embed, padding_idx=0, max_norm=None, norm_type=2)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # The linear part\n",
    "        eL = self.embeddingL(X)\n",
    "        logitL = eL.sum(dim=1)\n",
    "        \n",
    "        # The Quadratic-FM part using the O(kn) formulation from Steffen Rendle\n",
    "        eQ = self.embeddingQ(X)\n",
    "        logitFM1 = eQ.mul(eQ).sum(1).sum(2)\n",
    "        z = eQ.sum(dim=1)# sum across features\n",
    "        z2 = z.mul(z) # element-wise product\n",
    "        logitFM2 = z2.sum(dim=2) # sum across embedding dimensions\n",
    "        logitFM = (logitFM1 - logitFM2)*0.5\n",
    "        \n",
    "        # Total logit\n",
    "        logit = (logitL + logitFM).squeeze(dim=-1).squeeze(dim=-1)\n",
    "        logit+= self.B.expand(1, logit.size()[0]).transpose(0,1)\n",
    "        \n",
    "        if self.isClassifier:\n",
    "            return F.sigmoid(logit)\n",
    "        else:\n",
    "            return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FactorizationMachine(100, dim_embed=50, isClassifier=True, withCuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    X = X.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-04 *\n",
       "  0.0002\n",
       "  5.1449\n",
       "[torch.cuda.FloatTensor of size 2 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#loss_function = nn.NLLLoss()\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3) (1, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "if True: # Dummy data\n",
    "    train_input = np.array( [ [[11,20,4],[30,10,20],] ] )\n",
    "    train_target = np.array( [ [[1],[0],] ] )\n",
    "    print(train_input.shape, train_target.shape)\n",
    "else:\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris(False)\n",
    "\n",
    "    # Only keep classes 0 and 1 for binary classification example\n",
    "    data = iris['data'][iris['target']<2]\n",
    "    target = iris['target'][iris['target']<2]\n",
    "\n",
    "    # Quantile representation of the data : like a sparse vector representation\n",
    "    import pandas as pd\n",
    "    offset = 1\n",
    "    qs = 3\n",
    "    cols = []\n",
    "    for j in range(data.shape[1]):\n",
    "        col = pd.qcut(data[:,j], qs, labels=False)+offset\n",
    "        cols.append(col)\n",
    "        offset+= qs\n",
    "\n",
    "    qdata = np.vstack(cols).T\n",
    "    \n",
    "    # Shuffle\n",
    "    p = np.random.permutation(data.shape[0])\n",
    "    train_input = qdata[p]\n",
    "    train_target = target[p]\n",
    "    # WARNING : need batching !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_batches = 1\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx in range(nb_batches):\n",
    "        data = torch.LongTensor(train_input[batch_idx,:].astype('int'))\n",
    "        target = torch.LongTensor(np.squeeze(train_target[batch_idx,:].astype('int'))) # WARNING : squeeze\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #print(data.size())\n",
    "        print(target.size())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) # same thing as 'model.forward(data)' ?\n",
    "        print(output.size())\n",
    "        # INFO : still needs to debug these lines\n",
    "        #loss = loss_function(output, target)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        #if ((batch_idx % 10 == 0) or (batch_idx == nb_batches-1)):\n",
    "        #    print('Train Epoch: {} [{}]\\tLoss: {:.6f}'.format(\n",
    "        #        epoch, batch_idx * len(data), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for e in range(1):\n",
    "    train(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
